"""
Utility helpers for generating a Reddit user persona.

Functions
---------
extract_username(url: str) -> str | None
    Extract the username from a Reddit profile URL.

scrape_user_data(username: str, limit: int = 30)
    Return the user's recent posts and comments.

generate_persona(posts: list[dict], comments: list[dict]) -> str
    Use Google Gemini to build a persona with citations.

save_persona(username: str, persona: str) -> None
    Persist the generated persona to a text file.
"""

from __future__ import annotations

import os
import re
from typing import Tuple, List, Dict

import google.generativeai as genai
import praw
from dotenv import load_dotenv

# --------------------------------------------------------------------------- #
# Environment & client setup
# --------------------------------------------------------------------------- #

load_dotenv()  # Loads GOOGLE_API_KEY, REDDIT_CLIENT_ID, etc.

# Gemini
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
_GEMINI_MODEL = genai.GenerativeModel("models/gemini-1.5-flash")

# Reddit (PRAW)
_reddit = praw.Reddit(
    client_id=os.getenv("REDDIT_CLIENT_ID"),
    client_secret=os.getenv("REDDIT_CLIENT_SECRET"),
    user_agent="persona-generator-script",
)

# --------------------------------------------------------------------------- #
# Public helpers
# --------------------------------------------------------------------------- #


def extract_username(url: str) -> str | None:
    """
    Extract a Reddit username from the supplied profile URL.

    Parameters
    ----------
    url : str
        Full profile URL, e.g. ``https://www.reddit.com/user/example/``.

    Returns
    -------
    str | None
        The username portion or ``None`` if no match is found.
    """
    match = re.search(r"reddit\.com/user/([^/]+)", url)
    return match.group(1) if match else None


def scrape_user_data(
    username: str, limit: int = 30
) -> Tuple[List[Dict[str, str]], List[Dict[str, str]]]:
    """
    Fetch a user's most recent posts and comments.

    Parameters
    ----------
    username : str
        Reddit username (without the ``u/`` prefix).
    limit : int, optional
        Maximum number of posts and comments to pull for each type.

    Returns
    -------
    tuple[list[dict], list[dict]]
        Two lists containing post dicts and comment dicts respectively.
    """
    user = _reddit.redditor(username)

    posts = [
        {
            "text": f"{submission.title}\n{submission.selftext}",
            "url": f"https://reddit.com{submission.permalink}",
        }
        for submission in user.submissions.new(limit=limit)
    ]

    comments = [
        {
            "text": comment.body,
            "url": f"https://reddit.com{comment.permalink}",
        }
        for comment in user.comments.new(limit=limit)
    ]

    return posts, comments


def generate_persona(posts: List[Dict[str, str]], comments: List[Dict[str, str]]) -> str:
    """
    Build a persona description (with citations) via Gemini.

    Notes
    -----
    To keep the token count manageable, the first 50 items are used.

    Returns
    -------
    str
        Formatted persona text.
    """
    # Flatten content into prompt snippets
    snippets = [
        f"{item['text']}\nSource: {item['url']}" for item in (posts + comments)
    ][:50]

    if not snippets:
        return "No content available to generate a persona."    
    prompt = f"""
    You are an AI tasked with analyzing a Reddit user's personality based on their recent posts and comments.

    Generate a well-formatted TEXT-ONLY persona report (not markdown or HTML). Use plain formatting with:
    - Emoji headers (e.g. 🎯 Goals & Needs)
    - Clear section titles with dashes or lines
    - No asterisks, hashes, or markdown symbols
    - Indent or format citations clearly

    Each section should include relevant insights with citations from the user's content.

    Sections to include:
    1. 🎯 Interests
    2. 🤔 Personality Traits
    3. 🗣️ Tone of Writing
    4. 👨‍🎓 Profession or Education (if inferred)
    5. 😂 Humor or Style
    6. 🌎 Political/Social Leanings (if any)
    7. 🚫 Limitations
    8. 💬 Representative Quote (optional)
    9. ✅ Goals & Needs (optional)

    Here are their Reddit posts and comments:
    {'=' * 80}
    {chr(10).join(snippets)}
    """

    response = _GEMINI_MODEL.generate_content(prompt)
    return response.text


def save_persona(username: str, persona: str) -> None:
    """
    Save the generated persona to both .txt and .md files.

    Parameters
    ----------
    username : str
        Reddit username.
    persona : str
        The full text of the persona generated by the model.
    """
    txt_file = f"{username}_persona.txt"
    md_file = f"{username}_persona.md"

    with open(txt_file, "w", encoding="utf-8") as txt_handle:
        txt_handle.write(persona)

    markdown_version = convert_to_markdown(persona)

    with open(md_file, "w", encoding="utf-8") as md_handle:
        md_handle.write(markdown_version)

    print(f"✅ Persona saved as: {txt_file} and {md_file}")

def convert_to_markdown(text: str) -> str:
    """
    Convert a plain-text persona report into Markdown format.

    This includes:
    - Promoting emoji headers to Markdown-style headings.
    - Converting citation lines to link format.

    Parameters
    ----------
    text : str
        Plain text persona content.

    Returns
    -------
    str
        A Markdown-formatted version of the persona.
    """
    lines = text.splitlines()
    md_lines: list[str] = []

    section_emojis = (
        "🎯", "🤔", "🗣️", "👨‍🎓", "😂", "🌎", "🚫", "💬", "✅"
    )

    for line in lines:
        stripped = line.strip()

        if any(stripped.startswith(emoji) for emoji in section_emojis):
            md_lines.append(f"## {stripped}")
        elif "Source:" in line:
            parts = line.split("Source:")
            text_part = parts[0].strip()
            url_part = parts[1].strip()
            md_lines.append(
                f"- {text_part}  \n  **Source:** [{url_part}]({url_part})"
            )
        else:
            md_lines.append(line)

    return "\n".join(md_lines)
